{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06. Week 11-2 (211108)","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNqAFZ+ytNE11WcrVciv28q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fj4xCghVPr0S","executionInfo":{"status":"ok","timestamp":1636879337412,"user_tz":-540,"elapsed":30403,"user":{"displayName":"‍이호재[재학 / 영어학과]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02475819067827770201"}},"outputId":"bab082bd-d687-4a17-9195-769309cb43f0"},"source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HhaCnt_Oky54","executionInfo":{"status":"ok","timestamp":1636879357690,"user_tz":-540,"elapsed":20281,"user":{"displayName":"‍이호재[재학 / 영어학과]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02475819067827770201"}},"outputId":"90d19390-ffff-4df8-e890-8c640a0bedd4"},"source":["!pip install konlpy kss"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 6.2 MB/s \n","\u001b[?25hCollecting kss\n","  Downloading kss-3.3.1.1.tar.gz (42.4 MB)\n","\u001b[K     |████████████████████████████████| 42.4 MB 64 kB/s \n","\u001b[?25hCollecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n","Collecting beautifulsoup4==4.6.0\n","  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.9 MB/s \n","\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 70.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Collecting emoji\n","  Downloading emoji-1.6.1.tar.gz (170 kB)\n","\u001b[K     |████████████████████████████████| 170 kB 57.9 MB/s \n","\u001b[?25hBuilding wheels for collected packages: kss, emoji\n","  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kss: filename=kss-3.3.1.1-py3-none-any.whl size=42449239 sha256=405255a3e781f01f9543e51bcfac54804b22e0fd07c1ec54e3197b97276c4ab9\n","  Stored in directory: /root/.cache/pip/wheels/6e/9d/1d/52871154eff5273abb86b96f4f984c1cd67c5bde64239b060a\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=f96a38bf90f8ffb6c5217f7ce98a74d799ccced8f89445653c43371783b73347\n","  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n","Successfully built kss emoji\n","Installing collected packages: JPype1, emoji, colorama, beautifulsoup4, kss, konlpy\n","  Attempting uninstall: beautifulsoup4\n","    Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 emoji-1.6.1 konlpy-0.5.2 kss-3.3.1.1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wtxY6jeNpHO6","executionInfo":{"status":"ok","timestamp":1636879866403,"user_tz":-540,"elapsed":955,"user":{"displayName":"‍이호재[재학 / 영어학과]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02475819067827770201"}},"outputId":"8a2eadaa-a1ee-4b52-8cbc-9c171b661a2a"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"uhlA1_CJkHFp"},"source":["### article 2\n","- 한국어 BOW\n","- tokenize 원하는 걸로\n","- sentence split\n","- 불용어: 조사, 문장부호\n","- 라인별로 doc로 만들어서 dtm 만들고 pandas에 넣는다\n","\n","### article2 vs 1, 3, 4, 5\n","- article2에 가장 비슷한 문서 찾을 것"]},{"cell_type":"code","metadata":{"id":"eIUitZRnWXgk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636879359221,"user_tz":-540,"elapsed":1534,"user":{"displayName":"‍이호재[재학 / 영어학과]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02475819067827770201"}},"outputId":"309b1d46-a751-4e68-edaa-0b40929c7326"},"source":["PATH = \"/gdrive/MyDrive/01. Programming/Practical NLP/data/Week 11/\"\n","\n","with open(PATH + \"article2.txt\") as f:\n","  data = f.read().splitlines()\n","\n","data = [i for i in data if len(i) != 0]\n","data"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['새 방역체계인 위드 코로나 과정에서 신종 코로나바이러스 감염증(코로나19) 4차 유행이 지속되는 가운데 7일 신규 확진자 수는 2200명대를 기록했다. 닷새 연속 2000명대다.',\n"," '중앙방역대책본부는 이날 0시 기준으로 신규 확진자가 2224명 증가해 누적 37만9935명으로 집계됐다고 밝혔다.',\n"," '전일(2248명)보다는 24명 줄었지만 지난 3일 이후 닷새 연속으로 2000명대를 유지하며 확산세를 이어가고 있는 상황이다. 지난달 30일(발표일 기준 10월 31일) 신규 확진자 2061명보다는 163명이 많다.',\n"," '단계적 일상회복 계획이 개시된 이달 1일 이후 전국적으로 활동과 모임 등이 급증한 것이 영향을 준 것으로 풀이된다. 쌀쌀해진 날씨로 인해 환기가 부족한 실내활동이 늘어난 점도 영향을 주고 있다.']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"QUqYsJzqkrGm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636879387964,"user_tz":-540,"elapsed":28745,"user":{"displayName":"‍이호재[재학 / 영어학과]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02475819067827770201"}},"outputId":"fa7a8b07-f98f-4952-dcaf-67d0e991d98d"},"source":["# 1. Tokenize\n","from konlpy.tag import Okt\n","from collections import OrderedDict, Counter\n","import kss\n","import copy\n","\n","okt = Okt()\n","\n","def tokenize(data):   # input: splitlines한 리스트\n","  data = [line.strip() for line in data]\n","  data = kss.split_sentences(' '.join(data))\n","  tagged = [okt.pos(i) for i in data]\n","  tokens = [[i for i, j in sent if j != \"Josa\"] for sent in tagged]\n","  return tokens   # 문장별(kss splitted) 토큰 반환\n","\n","def create_lexicon(tokens):\n","  \"\"\"\n","  - input: tokenize(data)한 결과값\n","  - output: tokens의 lexicon\n","  \"\"\"\n","  lexicon = []\n","  for line in tokens:\n","    lexicon.extend(line)\n","  lexicon = set(lexicon)\n","  return lexicon\n","\n","tokens = tokenize(data)\n","lexicon = create_lexicon(tokens)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[Korean Sentence Splitter]: Initializing Pynori...\n"]}]},{"cell_type":"code","metadata":{"id":"DYj-vlRRpJ_E","executionInfo":{"status":"ok","timestamp":1636879387965,"user_tz":-540,"elapsed":22,"user":{"displayName":"‍이호재[재학 / 영어학과]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02475819067827770201"}}},"source":["def vectorize(tokens, lexicon):\n","  \"\"\"\n","  - input: tokenize(data)와 참고할 lexicon을 input으로 받음\n","  - output:lexicon에 대해서 frequency를 나타내는 OrderedDict 객체반환\n","  \"\"\"\n","  result = []\n","  zero_vector = OrderedDict((token, 0) for token in sorted(lexicon))\n","  token_count = [Counter(t) for t in tokens]\n","  for line in token_count:\n","    copy_vector = copy.copy(zero_vector)\n","    for key, value in line.items():\n","      copy_vector[key] = value\n","    result.append(copy_vector)\n","\n","  return result\n","\n","vector = vectorize(tokens, lexicon)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"VNbgnzEMqhci","executionInfo":{"status":"ok","timestamp":1636879387966,"user_tz":-540,"elapsed":22,"user":{"displayName":"‍이호재[재학 / 영어학과]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02475819067827770201"}}},"source":["import pandas as pd\n","\n","def create_dtm(vector):\n","  \"\"\"\n","  - input: lexicon을 기반으로 vectorize한 vector\n","  - output: dtm (여기서는 7 by 100 matrix, where 7 is the number of sentences from the input, 100 is the lexicon size)\n","  \"\"\"\n","  dtm = [[value for _, value in line.items()] for line in vector]\n","  # print([value for _, value in line.items()])   # DTM: Document Term Matrix(문서-단어 행렬)\n","  return dtm\n","\n","def create_df(vector):\n","  return pd.DataFrame(vector, columns=vector[0].keys())\n","\n","# create_df(vector)\n","dtm = create_dtm(vector)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3MhjLker3Bb","executionInfo":{"status":"ok","timestamp":1636879387966,"user_tz":-540,"elapsed":21,"user":{"displayName":"‍이호재[재학 / 영어학과]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02475819067827770201"}},"outputId":"d08514b5-9f74-4394-c5f6-fe343b5673d1"},"source":["print(tokens)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[['새', '방역', '체계', '위드', '코로나', '과정', '신종', '코로나바이러스', '감염증', '(', '코로나', '19', ')', '4', '차', '유행', '지속', '되는', '가운데', '7일', '신규', '확', '진자', '수', '2200', '명대', '기록', '했다', '.'], ['닷새', '연속', '2000', '명대', '.'], ['중앙', '방역', '대책', '본부', '이', '날', '0시', '기준', '신규', '확', '진자', '2224', '명', '증가', '해', '누적', '37만', '9935', '명', '집계', '됐다고', '밝혔다', '.'], ['전일', '(', '2248', '명', ')', '24', '명', '줄었지만', '지난', '3일', '이후', '닷새', '연속', '2000', '명대', '유지', '하며', '확산', '세', '이', '어가', '있는', '상황', '.'], ['지난달', '30일', '(', '발표', '일', '기준', '10월', '31일', ')', '신규', '확', '진자', '2061', '명', '163', '명', '많다', '.'], ['단계', '적', '일상', '회복', '계획', '개시', '된', '이', '달', '1일', '이후', '전국', '적', '활동', '모임', '등', '급증', '것', '영향', '준', '것', '풀이', '된다', '.'], ['쌀쌀해진', '날씨', '인해', '환기', '부족한', '실내', '활동', '늘어난', '점도', '영향', '주고', '있다', '.']]\n"]}]},{"cell_type":"markdown","metadata":{"id":"uVmkKBDCo7Eb"},"source":["## 문서별 유사도 측정"]},{"cell_type":"code","metadata":{"id":"106ju7frurnV","executionInfo":{"status":"ok","timestamp":1636880388487,"user_tz":-540,"elapsed":521,"user":{"displayName":"‍이호재[재학 / 영어학과]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02475819067827770201"}}},"source":["def extract_top_twenty(tokens):\n","  \"\"\"\n","  - input: tokenize(data)한 tokens를 받음\n","  - ouput: 7개의 문장에 대해 top20 tokens로만 이루어진 tokenized된 이중리스트\n","  \"\"\"\n","  temp = sum(tokens, [])   # 이중리스트 해체\n","  fdist = nltk.FreqDist(temp).most_common(20)\n","  keys = [i[0] for i in fdist]\n","\n","  new_tokens = [[token for token in line if token in keys] for line in tokens]\n","  return new_tokens"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCnabhsfqduL","executionInfo":{"status":"ok","timestamp":1636880388802,"user_tz":-540,"elapsed":3,"user":{"displayName":"‍이호재[재학 / 영어학과]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02475819067827770201"}},"outputId":"33ec06b0-cf45-4009-8a9b-9d62fdde9bde"},"source":["extract_top_twenty(tokens)"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['방역', '코로나', '(', '코로나', ')', '신규', '확', '진자', '명대', '.'],\n"," ['닷새', '연속', '2000', '명대', '.'],\n"," ['방역', '이', '기준', '신규', '확', '진자', '명', '명', '.'],\n"," ['(', '명', ')', '명', '이후', '닷새', '연속', '2000', '명대', '이', '.'],\n"," ['(', '기준', ')', '신규', '확', '진자', '명', '명', '.'],\n"," ['적', '이', '이후', '적', '활동', '것', '영향', '것', '.'],\n"," ['활동', '영향', '.']]"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"yC9ngAsQlC6W","executionInfo":{"status":"ok","timestamp":1636881442811,"user_tz":-540,"elapsed":279,"user":{"displayName":"‍이호재[재학 / 영어학과]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02475819067827770201"}}},"source":["# Create a pipeline with all the aforementioned functions\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","PATH = \"/gdrive/MyDrive/01. Programming/Practical NLP/data/Week 11/\"\n","\n","def create_dtm_toptwenty(file:str):\n","  dir = PATH + \"{0}.txt\".format(file)\n","  with open(dir, \"r\") as f:\n","    data = f.read().splitlines()\n","  \n","  # 1. splitlines을 기준으로 data 생성 (1d list)\n","  data = [i for i in data if len(i) != 0]\n","\n","  # 2. kss split & tokenize 실행 (2d list)\n","  tokens = tokenize(data)\n","\n","  # 3. top20 tokens에 대해서만 tokenize된 결괏값 (2d list)\n","  tokens = extract_top_twenty(tokens)\n","\n","  # 4. tokens에 대해 lexicon 생성\n","  lexicon = create_lexicon(tokens)\n","\n","  # 5. lexicon에 맞게 vectorize한 OrderedDict 객체 생성\n","  vector = vectorize(tokens, lexicon)\n","\n","  # 6. 5를 기반으로 dtm 생성\n","  dtm = create_dtm(vector)\n","\n","  return dtm\n","\n","def compare(dtm1, dtm2):\n","  similarities = cosine_similarity(dtm1, dtm2)\n","\n","  # similarities: 7 by 7 2d list\n","  # this means, each sentence is compared and is returned a cosine similarity value\n","  max_values = [value.max() for value in similarities]\n","  score = np.array(max_values).mean()\n","    \n","  # return cosine_similarity(dtm1, dtm2)\n","  return score"],"execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDdXd-hgtuTu","executionInfo":{"status":"ok","timestamp":1636881613514,"user_tz":-540,"elapsed":5122,"user":{"displayName":"‍이호재[재학 / 영어학과]","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02475819067827770201"}},"outputId":"5be280dd-9708-4e35-f6ad-160e6a933c97"},"source":["dtm1, dtm2, dtm3, dtm4, dtm5 = create_dtm_toptwenty(\"article1\"), create_dtm_toptwenty(\"article2\"), create_dtm_toptwenty(\"article3\"), create_dtm_toptwenty(\"article4\"), create_dtm_toptwenty(\"article5\")\n","\n","for i in (dtm1, dtm3, dtm4, dtm5):\n","  score = compare(dtm2, i)\n","  print(score)   # article1 is most similar to article2"],"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6126504345079543\n","0.5623032000074704\n","0.5863972440987043\n","0.5619741030953044\n"]}]}]}